{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_book(url, output_file):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)\n",
    "\n",
    "def split_into_chapters(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    start_index = content.find(start_marker)\n",
    "    end_index = content.find(end_marker)\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        content = content[start_index + len(start_marker):end_index]\n",
    "\n",
    "    chapter_pattern = r'(CHAPTER [IVXLCDM]+[\\r\\n]+.+?[\\r\\n]+)'\n",
    "    chapters = re.split(chapter_pattern, content)\n",
    "\n",
    "    combined_chapters = []\n",
    "    for i in range(1, len(chapters), 2):\n",
    "        chapter_marker = chapters[i].strip()\n",
    "        chapter_content = chapters[i+1].strip() if i+1 < len(chapters) else ''\n",
    "        combined_chapters.append(f\"{chapter_marker}\\n{chapter_content}\")\n",
    "\n",
    "    return combined_chapters\n",
    "\n",
    "def save_chapters(chapters, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i, chapter in enumerate(chapters, start=1):\n",
    "        chapter_file = os.path.join(output_dir, f'Chapter_{i}.txt')\n",
    "        with open(chapter_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(chapter.strip())\n",
    "        print(f'Saved: {chapter_file}')\n",
    "\n",
    "def main():\n",
    "    book_url = 'https://www.gutenberg.org/cache/epub/74210/pg74210.txt'\n",
    "    download_path = 'downloaded_book.txt'\n",
    "    output_directory = 'chapters'\n",
    "\n",
    "    print('Downloading book...')\n",
    "    download_book(book_url, download_path)\n",
    "    print('Book downloaded.')\n",
    "\n",
    "    print('Splitting book into chapters...')\n",
    "    chapters = split_into_chapters(download_path)\n",
    "    print(f'Total chapters found: {len(chapters)}')\n",
    "\n",
    "    print('Saving chapters...')\n",
    "    save_chapters(chapters, output_directory)\n",
    "    print('All chapters saved.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TYPICAL EDITORIAL WORKFLOW\n",
    "\n",
    "\n",
    "1. Initial Assessment (Manuscript Evaluation)\n",
    "\n",
    "- Objective: Understand the overall structure, themes, and quality of the manuscript.\n",
    "- Activities:\n",
    "    - Read through the entire manuscript to get a sense of the story, argument, or content.\n",
    "    - Identify major strengths and weaknesses.\n",
    "    - Provide a broad overview of what works well and what needs significant changes.\n",
    "\n",
    "2. Developmental Editing (Substantive Editing)\n",
    "\n",
    "- Objective: Address the big-picture elements such as structure, plot, character development, and overall coherence.\n",
    "- Activities:\n",
    "    - Work with the author to reorganize content, if necessary.\n",
    "    - Suggest additions, deletions, or modifications to improve flow and clarity.\n",
    "    - Ensure the manuscript has a logical structure and that the content is well-developed.\n",
    "    - Focus on pacing, plot holes, character arcs, and thematic consistency.\n",
    "\n",
    "3. Content Editing (Line Editing)\n",
    "\n",
    "- Objective: Refine the writing style, enhance clarity, and improve readability.\n",
    "- Activities:\n",
    "    - Edit for sentence structure, word choice, and tone.\n",
    "    - Ensure consistency in style and voice throughout the manuscript.\n",
    "    - Clarify ambiguous or confusing passages.\n",
    "    - Improve the overall flow of the text.\n",
    "\n",
    "4. Copyediting\n",
    "\n",
    "- Objective: Correct grammar, punctuation, spelling, and syntax errors.\n",
    "- Activities:\n",
    "    - Check for grammatical errors and correct them.\n",
    "    - Fix punctuation and spelling mistakes.\n",
    "    - Ensure consistency in usage (e.g., American vs. British English).\n",
    "    - Verify proper use of capitalization, hyphenation, and abbreviations.\n",
    "    - Fact-checking and verifying the accuracy of information (if necessary).\n",
    "\n",
    "5. Proofreading\n",
    "\n",
    "- Objective: Catch any remaining errors and ensure the final text is polished.\n",
    "- Activities:\n",
    "    - Perform a final review to catch any overlooked grammatical, punctuation, or spelling errors.\n",
    "    - Ensure formatting consistency (headings, fonts, spacing).\n",
    "    - Verify that all corrections from previous editing stages have been implemented.\n",
    "    - Check for consistency in layout and design elements (if applicable).\n",
    "\n",
    "6. Final Review\n",
    "\n",
    "- Objective: Ensure the manuscript is ready for publication or submission.\n",
    "- Activities:\n",
    "    - Perform a last read-through to ensure everything is in order.\n",
    "    - Address any final concerns or questions from the author.\n",
    "    - Prepare the manuscript for the final format (print, digital, etc.).\n",
    "\n",
    "### Detailed Step-by-Step Process\n",
    "\n",
    "1. Initial Assessment (Manuscript Evaluation)\n",
    "\n",
    "- Conduct a thorough read-through of the manuscript.\n",
    "- Provide a comprehensive editorial letter summarizing initial thoughts and suggestions.\n",
    "- Discuss the evaluation with the author and agree on the scope of developmental editing.\n",
    "\n",
    "2. Developmental Editing\n",
    "\n",
    "- Break down the manuscript into sections/chapters and evaluate each part.\n",
    "- Work on restructuring and reorganizing content for better flow and coherence.\n",
    "- Provide detailed feedback and suggestions for improving plot, character development, and pacing.\n",
    "- Collaborate with the author on revisions and ensure alignment with the overall vision.\n",
    "\n",
    "3. Content Editing (Line Editing)\n",
    "\n",
    "- Edit each sentence for clarity, conciseness, and readability.\n",
    "- Ensure consistency in tone, style, and voice throughout the manuscript.\n",
    "- Focus on improving dialogue, descriptions, and narrative flow.\n",
    "- Make suggestions for enhancing the writing style and eliminating redundancies.\n",
    "\n",
    "4. Copyediting\n",
    "\n",
    "- Review the manuscript line-by-line for grammatical, punctuation, and spelling errors.\n",
    "- Ensure adherence to a specific style guide (e.g., Chicago Manual of Style, APA).\n",
    "- Verify factual information and correct inconsistencies.\n",
    "- Implement standardized formatting and usage conventions.\n",
    "\n",
    "5. Proofreading\n",
    "\n",
    "- Conduct a meticulous review to catch any remaining errors or typos.\n",
    "- Verify the consistency of formatting elements (e.g., chapter headings, page numbers).\n",
    "- Ensure all previous corrections have been accurately implemented.\n",
    "- Prepare a final proof for the author’s approval.\n",
    "\n",
    "6. Final Review\n",
    "\n",
    "- Perform a last comprehensive read-through.\n",
    "- Address any remaining concerns or final touches requested by the author.\n",
    "- Ensure the manuscript is formatted correctly for its intended publication medium.\n",
    "- Prepare and submit the final version for publication or submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROJECT PLAN: PHASE1 - INITIAL ASSESSMENT\n",
    "\n",
    "1. Text Summarization Models\n",
    "These models can generate concise summaries of larger texts, maintaining essential information and context.\n",
    "\n",
    "Models and Techniques:\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformers): Effective for abstractive summarization, generating summaries that are concise yet informative.\n",
    "T5 (Text-to-Text Transfer Transformer): Versatile model that can be fine-tuned for summarization tasks, capable of producing high-quality summaries.\n",
    "PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization): Pre-trained specifically for summarization tasks, PEGASUS excels in generating coherent and contextually rich summaries.\n",
    "SummaRuNNer: A neural network-based model that focuses on extractive summarization, selecting the most important sentences to include in the summary.\n",
    "\n",
    "2. Topic Modeling\n",
    "These models help identify and extract key themes and topics from the text, providing a structured way to summarize content.\n",
    "\n",
    "Models and Techniques:\n",
    "\n",
    "LDA (Latent Dirichlet Allocation): A generative statistical model that helps identify topics within a text corpus, summarizing content by key themes.\n",
    "BERTopic: Utilizes BERT embeddings for topic modeling, capturing context and generating coherent topics.\n",
    "NMF (Non-Negative Matrix Factorization): Another approach for extracting topics from the text, useful for understanding the main themes.\n",
    "\n",
    "3. Key Phrase Extraction\n",
    "These models identify and extract the most important phrases and concepts from a text, providing a condensed version of the content.\n",
    "\n",
    "Models and Techniques:\n",
    "\n",
    "RAKE (Rapid Automatic Keyword Extraction): A straightforward and effective method for extracting key phrases from text.\n",
    "YAKE (Yet Another Keyword Extractor): Focuses on extracting keywords from individual documents without requiring a corpus for training.\n",
    "KeyBERT: Uses BERT embeddings to find keywords and key phrases that are contextually relevant.\n",
    "\n",
    "4. Information Retrieval Models\n",
    "These models can help identify the most relevant sections of the text, focusing the analysis on high-value content.\n",
    "\n",
    "Models and Techniques:\n",
    "\n",
    "BM25 (Best Matching 25): A ranking function used in search engines to estimate the relevance of documents based on query terms.\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): A statistical measure to evaluate the importance of a word in a document relative to a corpus, useful for identifying key sections.\n",
    "\n",
    "5. Hierarchical Attention Networks (HAN)\n",
    "HAN models can help summarize documents by focusing on the hierarchical structure of the text (e.g., sentences within paragraphs, paragraphs within chapters).\n",
    "\n",
    "Models and Techniques:\n",
    "\n",
    "Hierarchical Attention Networks: These models apply attention mechanisms at different levels (words, sentences) to capture the hierarchical structure of the text, summarizing while preserving context.\n",
    "Integrating Models into the Workflow\n",
    "To integrate these models effectively, you can design a multi-step pipeline:\n",
    "\n",
    "Initial Text Processing:\n",
    "\n",
    "Use Key Phrase Extraction (e.g., KeyBERT) to identify the most important phrases and concepts.\n",
    "Apply Topic Modeling (e.g., BERTopic) to understand the main themes.\n",
    "Summarization:\n",
    "\n",
    "Use Text Summarization Models (e.g., BART, PEGASUS) to generate concise summaries of individual sections or chapters.\n",
    "Combine these summaries to form a comprehensive overview of the manuscript.\n",
    "Contextual Refinement:\n",
    "\n",
    "Apply Hierarchical Attention Networks (HAN) to maintain the hierarchical context while further condensing the text.\n",
    "Information Retrieval:\n",
    "\n",
    "Use models like BM25 to rank and select the most relevant sections of the text for detailed analysis by the primary editor AI.\n",
    "\n",
    "### Example Workflow\n",
    "- Chunk the Manuscript: Divide the manuscript into chapters or sections.\n",
    "- Extract Key Phrases: Use KeyBERT to identify important phrases in each section.\n",
    "- Topic Modeling: Apply BERTopic to understand and summarize the main themes.\n",
    "- Summarize: Use BART to generate summaries of each section.\n",
    "- Hierarchical Refinement: Apply HAN to combine and refine these summaries.\n",
    "- Select Relevant Sections: Use BM25 to rank the most important sections for detailed analysis.\n",
    "- Primary Editing: Feed the condensed, contextually rich text into the primary editor AI for in-depth analysis and feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keybert import KeyBERT\n",
    "\n",
    "def extract_key_phrases(text, model, num_phrases=10):\n",
    "    key_phrases = model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=num_phrases)\n",
    "    return [phrase for phrase, score in key_phrases]\n",
    "\n",
    "def process_chapters(input_dir, output_file, num_phrases=10):\n",
    "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "    chapters_key_phrases = {}\n",
    "\n",
    "    for chapter_file in os.listdir(input_dir):\n",
    "        if chapter_file.endswith('.txt'):\n",
    "            chapter_path = os.path.join(input_dir, chapter_file)\n",
    "            with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            key_phrases = extract_key_phrases(text, model, num_phrases)\n",
    "            chapters_key_phrases[chapter_file] = key_phrases\n",
    "            print(f'Extracted key phrases for {chapter_file}')\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for chapter, phrases in chapters_key_phrases.items():\n",
    "            file.write(f'{chapter}:\\n')\n",
    "            for phrase in phrases:\n",
    "                file.write(f'  - {phrase}\\n')\n",
    "            file.write('\\n')\n",
    "\n",
    "    print(f'Key phrases saved to {output_file}')\n",
    "\n",
    "def main():\n",
    "    input_directory = 'chapters'\n",
    "    output_file = 'key_phrases.txt'\n",
    "    num_phrases = 10\n",
    "\n",
    "    process_chapters(input_directory, output_file, num_phrases)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keybert import KeyBERT\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def extract_key_phrases(text, model, num_phrases=10):\n",
    "    key_phrases = model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=num_phrases)\n",
    "    # Extract Named Entities\n",
    "    named_entities = extract_named_entities(text)\n",
    "    # Combine Key Phrases with Named Entities\n",
    "    combined_phrases = set(phrase for phrase, score in key_phrases) | set(named_entities)\n",
    "    return list(combined_phrases)\n",
    "\n",
    "def extract_topics(texts):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "    topic_model = BERTopic(vectorizer_model=vectorizer)\n",
    "    topics, _ = topic_model.fit_transform(texts)\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    return topic_info\n",
    "\n",
    "def process_chapters(input_dir, output_file, num_phrases=10):\n",
    "    # model = KeyBERT(TransformerDocumentEmbeddings('distilbert-base-nli-mean-tokens'))\n",
    "    model = KeyBERT('all-MiniLM-L6-v2')\n",
    "\n",
    "    chapters_key_phrases = {}\n",
    "    chapters_texts = []\n",
    "\n",
    "    for chapter_file in os.listdir(input_dir):\n",
    "        if chapter_file.endswith('.txt'):\n",
    "            chapter_path = os.path.join(input_dir, chapter_file)\n",
    "            with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            key_phrases = extract_key_phrases(text, model, num_phrases)\n",
    "            chapters_key_phrases[chapter_file] = key_phrases\n",
    "            chapters_texts.append(text)\n",
    "            print(f'Extracted key phrases and entities for {chapter_file}')\n",
    "\n",
    "    topic_info = extract_topics(chapters_texts)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for chapter, phrases in chapters_key_phrases.items():\n",
    "            file.write(f'{chapter}:\\n')\n",
    "            for phrase in phrases:\n",
    "                file.write(f'  - {phrase}\\n')\n",
    "            file.write('\\n')\n",
    "        file.write('Topics:\\n')\n",
    "        file.write(str(topic_info))\n",
    "\n",
    "    print(f'Key phrases and topics saved to {output_file}')\n",
    "\n",
    "def main():\n",
    "    input_directory = 'chapters'\n",
    "    output_file = 'key_phrases_and_topics.txt'\n",
    "    num_phrases = 10\n",
    "\n",
    "    process_chapters(input_directory, output_file, num_phrases)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def extract_key_phrases_tfidf(text, num_phrases=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=num_phrases)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    sorted_phrases = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_phrases = [phrase for phrase, score in sorted_phrases[:num_phrases]]\n",
    "\n",
    "    return top_phrases\n",
    "\n",
    "def summarize_with_context(text, key_phrases):\n",
    "    context_summaries = {}\n",
    "    for phrase in key_phrases:\n",
    "        prompt = f\"Summarize the context and importance of the following key phrase: {phrase}\\n\\n{text}\"\n",
    "        summary = summarizer(prompt, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
    "        context_summaries[phrase] = summary\n",
    "    return context_summaries\n",
    "\n",
    "def load_chapter(chapter_file, input_directory):\n",
    "    if chapter_file.endswith('.txt'):\n",
    "        chapter_path = os.path.join(input_directory, chapter_file)\n",
    "        with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "def clean_combined_phrases(combined_phrases):\n",
    "    cleaned_phrases = []\n",
    "    for phrase in combined_phrases:\n",
    "        cleaned_phrase = phrase.replace('\\n', ' ').strip()\n",
    "        cleaned_phrases.append(cleaned_phrase)\n",
    "    return cleaned_phrases\n",
    "\n",
    "def process_chapter(chapter_content, num_phrases=10):\n",
    "    tfidf_phrases           = extract_key_phrases_tfidf(chapter_content, num_phrases)\n",
    "    named_entities          = extract_named_entities(chapter_content)\n",
    "\n",
    "    combined_phrases        = list(set(tfidf_phrases) | set(named_entities))\n",
    "    combined_phrases        = clean_combined_phrases(combined_phrases)\n",
    "\n",
    "    print(f\"combined phrases: {combined_phrases}\")\n",
    "\n",
    "    contextual_summaries    = summarize_with_context(chapter_content, combined_phrases)\n",
    "\n",
    "    return contextual_summaries\n",
    "\n",
    "def save_summary(output_dir, chapter_file, contextual_summaries):\n",
    "    output_file = os.path.join(output_dir, f'{os.path.splitext(chapter_file)[0]}_key_phrases_with_context.txt')\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for phrase, context in contextual_summaries.items():\n",
    "            file.write(f'{phrase}: {context}\\n')\n",
    "\n",
    "def main():\n",
    "    input_directory = 'chapters'\n",
    "    output_dir      = 'chapter_outputs'\n",
    "    num_phrases     = 10\n",
    "\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "    for chapter_file in os.listdir(input_directory):\n",
    "        chapter_content = load_chapter(chapter_file, input_directory)\n",
    "        print(f\"loaded {chapter_file}...\")\n",
    "        summary = process_chapter(chapter_content, num_phrases)\n",
    "        print(f\"summarized {chapter_file}...\")\n",
    "        save_summary(output_dir, chapter_file, summary)\n",
    "        print(f\"saved summary for {chapter_file}\")\n",
    "        break\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def load_chapter(chapter_file, input_directory):\n",
    "    if chapter_file.endswith('.txt'):\n",
    "        chapter_path = os.path.join(input_directory, chapter_file)\n",
    "        with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "def summarize_chapter(chapter_content):\n",
    "    if len(chapter_content) < 100:\n",
    "        return \"Content too short to summarize.\"\n",
    "    \n",
    "    try:\n",
    "        summary = summarizer(chapter_content, max_length=200, min_length=100, do_sample=False)[0]['summary_text']\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return \"Error generating summary.\"\n",
    "\n",
    "def save_summary(output_dir, chapter_file, summary):\n",
    "    output_file = os.path.join(output_dir, f'{os.path.splitext(chapter_file)[0]}_summary.txt')\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(summary)\n",
    "\n",
    "def main():\n",
    "    input_directory = 'chapters'\n",
    "    output_dir      = 'chapter_outputs'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for chapter_file in os.listdir(input_directory):\n",
    "        chapter_content = load_chapter(chapter_file, input_directory)\n",
    "        print(f\"Loaded {chapter_file}...\")\n",
    "        summary = summarize_chapter(chapter_content)\n",
    "        print(f\"Summarized {chapter_file}...\")\n",
    "        save_summary(output_dir, chapter_file, summary)\n",
    "        print(f\"Saved summary for {chapter_file}\")\n",
    "        break\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "def load_chapter(chapter_file, input_directory):\n",
    "    if chapter_file.endswith('.txt'):\n",
    "        chapter_path = os.path.join(input_directory, chapter_file)\n",
    "        with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "def split_and_classify_paragraphs(text):\n",
    "    raw_paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n",
    "\n",
    "    uncertain_blocks = []\n",
    "    definite_paragraphs = []\n",
    "\n",
    "    for para in raw_paragraphs:\n",
    "        sub_paragraphs = para.split('\\n')\n",
    "\n",
    "        combined_para = \" \".join([sub.strip() for sub in sub_paragraphs if len(sub.strip()) > 40 or sub.strip().endswith(('.', '!', '?'))])\n",
    "\n",
    "        if combined_para:\n",
    "            sentence_count = combined_para.count('.')\n",
    "            if sentence_count == 0 or sentence_count == 1:\n",
    "                uncertain_blocks.append(combined_para)\n",
    "            else:\n",
    "                definite_paragraphs.append(combined_para)\n",
    "\n",
    "    return uncertain_blocks, definite_paragraphs\n",
    "\n",
    "def remove_annotations(text):\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\s*,\\s*', ' ', text)  # dangling commas\n",
    "    text = re.sub(r'\\s*\\[\\s*', ' ', text) # leftover open brackets\n",
    "    text = re.sub(r'\\s*\\]\\s*', ' ', text) # leftover close brackets\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # extra spaces and trim\n",
    "    return text\n",
    "\n",
    "def split_into_sentences(paragraph):\n",
    "    \"\"\"\n",
    "    Splits a paragraph into sentences.\n",
    "\n",
    "    :param paragraph: The paragraph to split into sentences.\n",
    "    :return: A list of sentences in the order they appear in the paragraph.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    return sentences\n",
    "\n",
    "chapter12_content               = load_chapter('chapter_name_here.txt', 'stuff')\n",
    "uncertain_blocks, paragraphs    = split_and_classify_paragraphs(chapter12_content)\n",
    "cleaned_paragraph               = remove_annotations(paragraphs[1:][0])\n",
    "sentences                       = split_into_sentences(cleaned_paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chapter12_content = clean_text(chapter12_content)  # Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "qa_model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "print('0')\n",
    "# qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "print('1')\n",
    "# qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "print('2')\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "print('3')\n",
    "def extract_fiction_elements(chapter_text):\n",
    "    \"\"\"\n",
    "    Extract core elements of a fiction chapter based on the fiction framework.\n",
    "    \n",
    "    Args:\n",
    "    - chapter_text (str): The text of the chapter to analyze.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the extracted elements (plot, characters, setting).\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    questions = {\n",
    "        \"plot\": \"What are the main events?\",\n",
    "        \"characters\": \"Who are the key characters?\",\n",
    "        \"setting\": \"Where does the story take place?\",\n",
    "        \"themes\": \"What are the central themes?\",\n",
    "        \"conflict\": \"What is the main conflict in this chapter?\",\n",
    "        \"resolution\": \"How is the conflict resolved?\"\n",
    "    }\n",
    "\n",
    "    extracted_elements = {}\n",
    "\n",
    "    i = 0\n",
    "    for key, question in questions.items():\n",
    "        print(f'extract_fiction_elements {i}')\n",
    "        result = qa_pipeline(question=question, context=chapter_text)\n",
    "        extracted_elements[key] = result['answer']\n",
    "        i += 1\n",
    "\n",
    "    return extracted_elements\n",
    "\n",
    "\n",
    "chapter_text = \"\"\"\n",
    "The wind howled through the trees as John trudged through the snow. He knew he was running out of time.\n",
    "The sun was setting, and with it, any hope of finding shelter. He could feel the weight of his journey \n",
    "bearing down on him. Suddenly, in the distance, he saw a faint light flickering. Summoning the last of \n",
    "his strength, he pushed forward, hoping it wasn't just a mirage...\n",
    "\"\"\"\n",
    "\n",
    "elements = extract_fiction_elements(chapter_text)\n",
    "print(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "secret = ''\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=secret,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a content editor with a specialty for fiction literature.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"\n",
    "You are part of an content editing team. This team is a fleet of specially trained GPT assistants. Each member on the team is designed to perform a specific function that in aggregate, would provide a comprehensive editorial service to an author of a full manuscript.\n",
    "\n",
    "Your sub-team focuses on implementing the codified rules of a given style guide. In this case, you are adhering to the Taylor & Francis style guide.\n",
    "\n",
    "Here are the specifics of your role:\n",
    "Title: Sentence Structure and Clarity\n",
    "\n",
    "Input:\n",
    "- you will be provided a json object with two keys: 'sentence' and 'rule'\n",
    "- the value of the 'sentence' key will be the sentence you are to check\n",
    "- the value of the 'rule' key will be the specific style rule you are to check the given sentence for\n",
    "\n",
    "Expectation(s):\n",
    "- check that the sentence obeys the given rule to check\n",
    "- if the rule is not obeyed by the sentence, rewrite the sentence so that it does, otherwise leave it unchanged\n",
    "\n",
    "Output:\n",
    "- json object with two keys: 'sentence' and 'edited'\n",
    "- the value of the 'sentence' key should be the given sentence. if it was not edited by you because it obeyed the rule, provide the same sentence back unaltered. if required edits because it did not obey the rule, provided the edited version so that it now obeys the given rule\n",
    "- no other content or data should be provided back besides the designated json output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Analyse the following chapter in order to provide a critique of the 5 pillars for a well structured story. Your focus for this pass is on \"Consistency\". The guiding question here is \"Does this chapeter Maintain a consistent tone and pacing throughout.\"\n",
    "\n",
    "Your response will be used by another system, so the structure and format of your respone is important to follow this framework:\n",
    "\n",
    "- Structure of the response should be a JSON object.\n",
    "- There should be a key in the JSON object called \"summary_score\" and it should give a score out of 10, judging as objectively as possible how much of the content is \"Consistent\" to the overall tone and pacing.\n",
    "- There should be one key per section in the content that needs to be improved to help bring the work to a higher \"consistency_score\". The key format should be the paragraph number being referenced.\n",
    "- The value for the above \"paragraph key\" should be a concise summary of what needs to be improved in this section to help bring up the consistency score. It should start with a single word prescription from these three options: Delete, Alter, Extend. Followed by a short and concise description of why this section should either be deleted, altered, or extended in order to improve overall consistency.\n",
    "- No additional content should be included in your response outside of the JSON object described in the instructions above\n",
    "\n",
    "Chapter content below:\n",
    "\n",
    "{chapter_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch12_summary = \"\"\"\n",
    "### Summary of Chapter 12: The New Economy and a Vision for India's Future Development\n",
    "\n",
    "**Core Thematic Elements:**\n",
    "1. **Vision for India’s Future Development**: The chapter introduces a vision for India's development, grounded in Social Market Economy principles, aiming to address challenges brought by digital technology and new economic developments. It emphasizes the complexity of implementing such a model in India's diverse and multifaceted socioeconomic landscape.\n",
    "\n",
    "2. **Global Technological Developments**: The chapter highlights the significant impact of recent technological advancements (e.g., AI, IoT, renewable energy) on socioeconomic changes. It discusses the opportunities and challenges posed by these technologies, including the need for an integrated approach to their adoption.\n",
    "\n",
    "3. **Strategic Futures**: A call for a transformation in institutions and mindsets is made to address the evolving challenges of the digital economy. The need for new development strategies, particularly in education, workforce training, and policy adaptation, is emphasized.\n",
    "\n",
    "4. **Comparative Perspectives**: The chapter compares India's development trajectory with that of other regions (Asia, Europe, the Americas), emphasizing the limitations of replicating models from other countries. It notes the importance of learning from global experiences while adapting them to India's unique context.\n",
    "\n",
    "5. **A Vision for India's Future Development**: The chapter outlines a vision for India's development that balances technological advancement with social equity, emphasizing the need for a cohesive and inclusive approach involving government, private sector, and civil society.\n",
    "\n",
    "6. **Lessons from Global Experiences**: It draws lessons from the social market economies of Germany, Nordic countries, and East Asia, suggesting that while these models offer valuable insights, they cannot be directly applied to India due to different historical and socioeconomic contexts.\n",
    "\n",
    "**General Style and Tone:**\n",
    "- The writing is analytical and forward-looking, combining a detailed examination of historical development models with a speculative discussion on the future.\n",
    "- The tone is scholarly, with a focus on policy implications and strategic recommendations.\n",
    "- The chapter balances optimism about technological progress with caution regarding its potential downsides.\n",
    "\n",
    "**Key Points:**\n",
    "- The implementation of a Social Market Economy in India faces challenges due to its diversity and political complexity.\n",
    "- Recent technological advancements have a transformative impact on socioeconomic development, requiring an integrated and multidimensional approach.\n",
    "- India’s development trajectory should consider global lessons but must be tailored to its unique challenges and opportunities.\n",
    "- A cohesive and inclusive development model, involving all societal stakeholders, is essential for India’s future.\n",
    "- Learning from the experiences of other regions is crucial, but India must adapt these lessons to its specific context.\n",
    "\n",
    "This summary provides the essential components required to understand the chapter's main arguments, thematic elements, and the overall approach, while also maintaining a focus on the broader implications for India's future development.\n",
    "\"\"\"\n",
    "\n",
    "ch12_paragraph1 = \"\"\"[Note: [Words???], [/consolance], (Mention cultural and other diversity, traits of selfish focus on individual and  family and limited willingness to sacrifice for the common good, barring many exceptions making it hard to generalise– cultural impediment – make social market economy paradigm unrealistic expect for certain pockets or type of development.)]This chapter presents a new economy a vision for India's future development vision for India grounded in Social Market Economy principles and challenges relating to digital technology and other new economic developments. The vision points to an ambitious, aspirational goal , though realising the complexity and diversity, political economy, and other issues in India's socioeconomic fabric makes it hard to build consensus on a new social market economy \"model\" in line  with the requirement of a new economy and even more to implement such a development model/paradigm.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_setup = \"You are part of a team of specially tuned AI Agents whose function is to edit content/literature. Your team specializes in academic literature review. Your specific role on this team is as follows:\"\n",
    "assistant1 = f\"\"\"\n",
    "{assistant_setup} summarize whole sections of a work and reduce it to its core components. The reason you perform this function is because you are prepping the content/section for the next member in your team of AI editors, reducing the content length to only the critical components required for understanding what a section is about. Your focus should be highlighting the core thematic elements, the general style and tone of writing, and the key points being made in the chapter/section.\"\"\"\n",
    "assistant2 = f\"\"\"\n",
    "{assistant_setup} you take a summary of a section that is prepared by a member of your team and you analyze a given paragraph from this section to check its \"Consistency\". The guiding questions here are \"Does this paragraph maintain a consistent tone and pacing throughout? Does it fit in to the chapter given our understanding of the chapter's contents and messaging\"\n",
    "\n",
    "Your response will be used by another system, so the structure and format of your response is important to follow this framework:\n",
    "\n",
    "- Structure of the response should be a JSON object.\n",
    "- There should be a key in the JSON object called \"score\" and it should give a score out of 10, judging as objectively as possible how much of the content is \"Consistent\" to the overall tone, pacing, and message.\n",
    "- There should be a key in the JSON object called \"comments\". The value for the \"comments\" key should be a concise summary of what needs to be improved in this section to help bring up the consistency \"score\". It should start with a single word prescription from these three options: Delete, Alter, Extend. Followed by a short and concise description of why this section should either be deleted, altered, or extended in order to improve overall consistency.\n",
    "- No additional content should be included in your response outside of the JSON object described in the instructions above\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_assistant2_prompt(summary, paragraph):\n",
    "    prompt = f\"\"\"\n",
    "    Summary of content below:\n",
    "    {summary}\n",
    "\n",
    "    Paragraph to be analyzed below:\n",
    "    {paragraph}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraphs[1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": assistant2},\n",
    "            {\"role\": \"user\", \"content\": craft_assistant2_prompt(ch12_summary, paragraph)}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    results[f'paragraph_{i+1}'] = chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass1 = {\n",
    "    \"consistency_score\": 6,\n",
    "    \"1\": \"Alter. The introduction is clear but slightly verbose, which affects the pacing. Simplifying the language and cutting unnecessary phrases can help maintain a more consistent tone.\",\n",
    "    \"3\": \"Alter. The paragraph starts strong but introduces several ideas that deviate slightly from the established tone, creating a sense of rushed pacing. Simplifying the language and focusing on one or two main points can improve consistency.\",\n",
    "    \"4\": \"Extend. The transition between discussing global ICT changes and India's specific context is abrupt. A few additional sentences to bridge these ideas would improve the flow.\",\n",
    "    \"5\": \"Extend. The section on India's recent achievements and vision could be expanded slightly to provide smoother transitions and maintain the pacing established in earlier paragraphs.\",\n",
    "    \"6\": \"Alter. The paragraph introduces too many new concepts without adequate connection to earlier content. Revising the structure to focus on fewer points would enhance consistency.\",\n",
    "    \"7\": \"Delete. This paragraph repeats information already mentioned earlier in the chapter. Removing it will streamline the flow and reinforce the chapter's consistency.\",\n",
    "    \"8\": \"Extend. The mention of India’s socioeconomic challenges lacks depth, creating a disjointed feel. Expanding on this idea would help maintain the chapter's pacing.\",\n",
    "    \"9\": \"Alter. The discussion on creative and disruptive technologies starts well but then shifts abruptly to a broader analysis. A more gradual transition or a split into two paragraphs might maintain the pacing better.\",\n",
    "    \"10\": \"Delete. The paragraph revisits themes already discussed, which disrupts the flow. Removing it will keep the chapter focused and consistent.\",\n",
    "    \"11\": \"Alter. The paragraph shifts to a more technical discussion, which can be jarring. Rewriting to align with the chapter's general tone will help maintain consistency.\",\n",
    "    \"12\": \"Extend. The mention of demographic changes is too brief and feels disconnected. Expanding on this idea and linking it to earlier content would help with consistency.\",\n",
    "    \"13\": \"Extend. The brief mention of India's economic standing lacks detail and disrupts the flow. Expanding on this idea or linking it more clearly to the previous content would enhance consistency.\",\n",
    "    \"14\": \"Alter. The tone becomes overly formal, which contrasts with the previous sections. Simplifying the language will help maintain a consistent tone throughout.\",\n",
    "    \"15\": \"Delete. The paragraph introduces redundant information already covered earlier, which affects pacing. Removing it will improve consistency.\",\n",
    "    \"16\": \"Alter. The discussion of international comparisons introduces a new analytical tone that doesn’t align with earlier content. Revising the language to match the rest of the chapter would help.\",\n",
    "    \"17\": \"Alter. This paragraph introduces complex concepts that shift the tone to a more academic and theoretical style. Simplifying the language or breaking it into two parts could help maintain a consistent tone.\",\n",
    "    \"18\": \"Extend. The mention of ICT's impact on different sectors is brief and feels disconnected. Adding more detail would enhance the flow and consistency.\",\n",
    "    \"19\": \"Alter. The paragraph introduces complex concepts that disrupt the pacing. Simplifying the language or breaking it into two parts could help maintain a consistent tone.\",\n",
    "    \"20\": \"Delete. The paragraph is redundant with earlier content. Removing it will help streamline the chapter and maintain consistency.\",\n",
    "    \"21\": \"Alter. The tone shifts to a more theoretical discussion, which contrasts with earlier content. Rewriting to align with the chapter’s overall tone will improve consistency.\",\n",
    "    \"22\": \"Delete. The analysis of India's potential future development scenarios becomes repetitive and diverges from the earlier tone. Removing redundant statements will keep the chapter focused and consistent.\",\n",
    "    \"23\": \"Alter. The conclusion introduces new ideas abruptly, which affects the flow. Revising the structure to summarize rather than introduce new concepts would enhance consistency.\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuraledit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
